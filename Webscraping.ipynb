{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Webscrape Human Genome Dataset__\n",
    "\n",
    "- In order to map the population codes to each individual sample I went on to the Genome_Website and scraped the population samples along with their corresponding population code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/AlexGaujean/anaconda3/lib/python3.7/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /Users/AlexGaujean/anaconda3/lib/python3.7/site-packages (from selenium) (1.24.1)\n",
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/AlexGaujean/anaconda3/lib/python3.7/site-packages (from bs4) (4.6.3)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/AlexGaujean/Library/Caches/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install necessary modules\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save variables for each row in our main population genome table  \n",
    "page = requests.get('http://www.internationalgenome.org/cell-lines-and-dna-coriell')\n",
    "soup = BeautifulSoup(page.content, 'lxml') \n",
    "\n",
    "popcode = soup.find('tr')\n",
    "popcode = soup.findAll('tr')\n",
    "#collect the more specific population descriptions \n",
    "population_descriptions = [x.td.text for x in popcode[1:]]\n",
    "#collect URLs for individual sample names\n",
    "hrefs = [x.td.next_sibling.next_sibling.next_sibling.next_sibling.a['href'] for x in popcode[1:]]\n",
    "#collect the population codes for each of our subsamples\n",
    "population_code = [x.td.next_sibling.next_sibling.text for x in popcode[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FIN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a population_dict where we will store our dictionary values and zip that with the corresponding 'href' tags that we found online\n",
    "population_dict = {}\n",
    "zipped_list = zip(population_code, hrefs)\n",
    "list(zipped_list)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a for loop that will go through our reference genome website and scrape all of the individual sample accounts\n",
    "population_dict = {}\n",
    "zipped_list = zip(population_code, hrefs)\n",
    "#setup chrome driver to scrape the tables for population codes\n",
    "browser = webdriver.Chrome('/Users/AlexGaujean/Desktop/chromedriver')\n",
    "time.sleep(5)\n",
    "# loop through our list of links\n",
    "for i in zipped_list:\n",
    "    browser.get(i[1])\n",
    "    ## code for webscraping with selenium ##\n",
    "    table = browser.find_element_by_xpath('//*[@id=\"grdRef\"]')\n",
    "        #get all of the rows in the table\n",
    "    try: \n",
    "        rows = table.find_elements_by_tag_name('tr')\n",
    "        #setup list called sample which we are going to use to append our sample_codes\n",
    "        sample = []\n",
    "        for row in rows[1:]:\n",
    "            cols = row.find_elements_by_tag_name('td')\n",
    "            sample.append(cols[0].text)\n",
    "    except IndexError: \n",
    "        rows = table.find_elements_by_tag_name('tr')\n",
    "        sample = []\n",
    "        for row in rows[3:]:\n",
    "            cols = row.find_elements_by_tag_name('td')\n",
    "            sample.append(cols[0].text) \n",
    "    population_dict[i[0]] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out all the webscraped tables that skipped values \n",
    "def popcode_filter(dictionary_key): \n",
    "    dictionary_key = list(filter(lambda x: not (x == \" \"), dictionary_key))\n",
    "popcode_filter(population_dict['MXL'])\n",
    "popcode_filter(population_dict['ASW'])\n",
    "popcode_filter(population_dict['YRI']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Population description will equal the list of samples from our dictionary this way we can save them and map them onto our main file\n",
    "Finnish_in_Finland = population_dict['FIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
